{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoding Graphical User Interfaces\n",
    "\n",
    "In our initial approach to solving code2pix, we assumed we needed to pretrain a convolutional model that could decode textual data into images of graphical user interfaces. The purpose of training the following autoencoder is to train the decoder that we then use in the code2pix model. However, last minute experiments showed that training the autoencoder is **not strictly necessary** to train code2pix. If you would like to skip straight to code2pix, feel free to initialize the model, skip training, and just save the model for its architecture. Code2Pix will just needs the layer architecture constructed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (LSTM, Dense, Conv2D, Input, Reshape, concatenate, MaxPooling2D,Dropout, Flatten, \n",
    "RepeatVector, UpSampling2D, Conv1D, Permute, BatchNormalization, Activation, UpSampling2D)\n",
    "from keras import Model\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra Autoencoder\n",
    "\n",
    "Model that understands our synthetic datasets (android, ios, web) interfaces, using a shared CNN to encode each picture, and unique heads to decode for each platform (3 heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_guis(data_dir):\n",
    "    \"\"\"Loads in target data (picture data) in a given directory.\n",
    "    Returns a np.array of the data loaded.\"\"\"\n",
    "    GUIS = []\n",
    "    training_GUI_listing = os.listdir(data_dir)\n",
    "    for listing in training_GUI_listing:\n",
    "        if listing.endswith('.npz'):\n",
    "            temp = np.load(join(data_dir, listing))['features']\n",
    "            # reshape to be flat\n",
    "            GUIS += [temp]\n",
    "    GUIS = np.array(GUIS)\n",
    "    return GUIS\n",
    "\n",
    "def make_head():\n",
    "    \"\"\"Returns a standard decoder model\"\"\"\n",
    "    input_layer = Input(shape=(16, 16, 16))\n",
    "    last = Conv2D(128, (4, 4), padding='same', activation='relu')(input_layer)\n",
    "\n",
    "    last = BatchNormalization()(last)\n",
    "    last = UpSampling2D(size=(2, 2))(last)\n",
    "    last = Conv2D(96, (3, 3), padding='same', activation='relu')(last)\n",
    "    # model.add(Conv2DTranspose(96, (2, 2), strides=(2, 2)))\n",
    "    last = BatchNormalization()(last)\n",
    "    last = UpSampling2D(size=(4, 4))(last)\n",
    "    last = Conv2D(32, (3, 3), padding='same', activation='relu')(last)\n",
    "    # model.add(Conv2DTranspose(32, (2, 2), strides=(2, 2)))\n",
    "    last = BatchNormalization()(last)\n",
    "    last = UpSampling2D(size=(2, 2))(last)\n",
    "    # model.add(Conv2D(16, (3, 3), padding='same', activation='relu'))\n",
    "    last = Conv2D(3, (3, 3), padding='same', activation='sigmoid')(last)\n",
    "    # model.add(Conv2DTranspose(color, (3, 3), strides=(2, 2)))\n",
    "    model = Model(input_layer, last)\n",
    "    return model\n",
    "    \n",
    "def make_hydra(loss='binary_crossentropy', optimizer='rmsprop'):\n",
    "    \"\"\"Creates 3 headed autoencoder model to handle the different datasets.\n",
    "    Returns [android, ios, web] models\"\"\"\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    input_layer = Input(shape=(256, 256, 3))\n",
    "    drop_rate = 0.25\n",
    "    last = Conv2D(64, (5, 5), strides=(2,2), activation='relu')(input_layer)\n",
    "    last = Conv2D(128, (4, 4), padding='same', strides=(1,1), activation='relu')(last)\n",
    "    last = MaxPooling2D(pool_size=(2,2))(last)\n",
    "    last = Dropout(drop_rate)(last)\n",
    "    last = Conv2D(128, (4, 4), padding='same', strides=(2,2), activation='relu')(last)\n",
    "    last = Conv2D(256, (3, 3), padding='same', strides=(1,1), activation='relu')(last)\n",
    "    last = Conv2D(128, (3, 3), padding='same', strides=(1,1), activation='relu')(last)\n",
    "    last = MaxPooling2D(pool_size=(2, 2))(last)\n",
    "    last = Dropout(drop_rate)(last)\n",
    "\n",
    "    last = Conv2D(128, (3, 3), padding='same', strides=(1,1), activation='relu')(last)\n",
    "    last = Conv2D(64, (3, 3), padding='same', strides=(1,1), activation='relu')(last)\n",
    "    last = Conv2D(16, (3, 3), padding='same', strides=(1,1), activation='relu')(last)\n",
    "\n",
    "    encoder = Model(input_layer, last)\n",
    "    android = make_head()\n",
    "    ios = make_head()\n",
    "    web = make_head()\n",
    "    \n",
    "    models = []\n",
    "    for model in [android, ios, web]:\n",
    "        head_spout = model(encoder.output)\n",
    "        head = Model(input_layer, head_spout)\n",
    "        head.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        models.append(head)\n",
    "    return models\n",
    "\n",
    "def train_hydra(epochs, models, datasets, val_split=0.2):\n",
    "    \"\"\"Assumes models=[android, ios, web], and datasets ordered alphabetically as well.\n",
    "    Trains the models sequentially, 1 epoch after the other. Validation split is last\n",
    "    val_split % of the data.\n",
    "    \n",
    "    ~16 epochs seems to give good perf, >30 pretty good\"\"\"\n",
    "    train = [data[:int((1 - val_split)*len(data))] for data in datasets]\n",
    "    validation = [data[int((1-val_split)*len(data)):] for data in datasets]\n",
    "    val_loss = 20\n",
    "    val_best = 20\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch: {}\".format(i+1))\n",
    "        val_loss = 0\n",
    "        for model, data, val in zip(models, train, validation):\n",
    "            model.fit(data, data, batch_size=16, validation_data=(val, val))\n",
    "            val_loss += model.evaluate(val, val)[0]\n",
    "        print(\"Model had avg val loss of: {}\".format(val_loss / 3))\n",
    "        if val_loss < val_best:\n",
    "            val_best = val_loss / 3\n",
    "        print(\"Best avg validation loss is {}\".format(val_best))\n",
    "            \n",
    "android_data = load_guis('android/training_features/')\n",
    "ios_data = load_guis('ios/training_features/')\n",
    "web_data = load_guis('web/training_features/')\n",
    "models = make_hydra()\n",
    "train_hydra(28, models, [android_data, ios_data, web_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the models' performance. Example visualizations are saved in `android_hydra.png`, `ios_hydra.png`, and `web_hydra.png` respectively. To change which pictures are shown, modify the `idx` argument passed to predict. Should be between \\[0, len(data) = 1500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, idx, name):\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(dataset[idx])\n",
    "    axes[1].imshow(model.predict(np.expand_dims(dataset[idx], 0))[0])\n",
    "    for ax in axes:\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        \n",
    "    fig.savefig(name, dpi=70)\n",
    "    plt.show()\n",
    "\n",
    "for model, data, name in zip(models, [android_data, ios_data, web_data], ['android', 'ios', 'web']):\n",
    "    predict(model, data, 100, '{}_hydra.png'.format(name))\n",
    "    model.save('{}-hydra-model'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that the models saved properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% ls *-model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Kernel",
   "language": "python",
   "name": "torchkern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
